{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading clik_list \n",
    "url_data=pd.read_excel(r\"C:\\Mba notes\\novels\\Intern Dataset\\Text Analysis\\cik_list.xlsx\")\n",
    "\n",
    "#loading master data\n",
    "master_data=pd.read_csv(r\"C:\\Mba notes\\novels\\Intern Dataset\\Text Analysis\\LoughranMcDonald_MasterDictionary_2018.csv\")\n",
    "\n",
    "#loading uncertainity and data \n",
    "uncertainity=pd.read_excel(r\"C:\\Mba notes\\novels\\Intern Dataset\\Text Analysis\\uncertainty_dictionary.xlsx\")\n",
    "constraining_dictionary=pd.read_excel(r\"C:\\Mba notes\\novels\\Intern Dataset\\Text Analysis\\constraining_dictionary.xlsx\")\n",
    "\n",
    "# making list of all url\n",
    "url=[]\n",
    "for i in url_data[\"SECFNAME\"]:\n",
    "    url.append(\"https://www.sec.gov/Archives/\"+i)\n",
    "    \n",
    "# getting words from dictionary \n",
    "Dict_words=[i for i in master_data[\"Word\"]]\n",
    "Dict_words=str(Dict_words)\n",
    "Dict_words=Dict_words.lower()\n",
    "\n",
    "# getting positive words\n",
    "df1=master_data[[\"Word\",\"Positive\",\"Negative\"]]\n",
    "positive_words=[]\n",
    "for i,j in zip(df1[\"Word\"],df1[\"Positive\"]):\n",
    "    if j!=0:\n",
    "        positive_words.append(i)\n",
    "positive_words=str(positive_words)\n",
    "positive_words=positive_words.lower()\n",
    "\n",
    "# getting negative words\n",
    "negative_words=[]\n",
    "for i,j in zip(df1[\"Word\"],df1[\"Negative\"]):\n",
    "    if j!=0:\n",
    "        negative_words.append(i)\n",
    "negative_words=str(negative_words)\n",
    "negative_words=negative_words.lower()\n",
    "\n",
    "# getting text from url\n",
    "def scrap(url):\n",
    "    u=url\n",
    "    webpage=requests.get(u)\n",
    "    soup=BeautifulSoup(webpage.text,\"html.parser\")\n",
    "    return soup\n",
    "txt1=scrap(url[0])\n",
    "txt1=str(txt1)\n",
    "txt2=txt1.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_score1=[]\n",
    "negative_score1=[]\n",
    "polarity_score1=[]\n",
    "average_sentence_length1=[]\n",
    "percentage_of_complex_words1=[]\n",
    "fog_index1=[]\n",
    "complex_word_count1=[]\n",
    "word_count1=[]\n",
    "uncertainty_score1=[]\n",
    "constraining_score1=[]\n",
    "positive_word_proportion1=[]\n",
    "negative_word_proportion1=[]\n",
    "uncertainty_word_proportion1=[]\n",
    "constraining_word_proportion1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0-20):\n",
    "\n",
    "        txt1=scrap(url[i])\n",
    "        txt1=str(txt1)\n",
    "        txt2=txt1.lower()\n",
    "        #cleaning the text\n",
    "        c1=re.sub(\"\\d\",\"\",txt2)\n",
    "        c1=re.sub(\"\\W\",\" \",c1)\n",
    "        c1=res = re.sub(' +', ' ', c1)\n",
    "        res=re.sub(r\"[^\\w\\s]\",\"\",c1)\n",
    "        res=re.sub(\"www\",\"\",res)\n",
    "        res=re.sub(\"______\",\"\",res)\n",
    "\n",
    "        # tokenization\n",
    "        tokens=word_tokenize(res)\n",
    "        w=[]\n",
    "\n",
    "        #Personal Pronouns\n",
    "        P=['I', 'we',' my',' ours','us']\n",
    "        Personal_Pronouns=[]\n",
    "        for i in P:\n",
    "            if i in txt1:\n",
    "                Personal_Pronouns.append(1)\n",
    "\n",
    "        #removing stopwords\n",
    "        for word in tokens:\n",
    "                if not word in stopwords.words(\"english\"):\n",
    "                    w.append(word)\n",
    "\n",
    "        # removing words greater than length 15 and less than 2\n",
    "        for i in w:\n",
    "            if len(i)>15 or len(i)<2:\n",
    "                w.remove(i)\n",
    "\n",
    "        # taking only those words which are in dictionary and our cleaned word\n",
    "        word=[]\n",
    "        for i in w:\n",
    "            if i in Dict_words:\n",
    "                word.append(i)\n",
    "\n",
    "\n",
    "        # lemmatizing based on pos_tag\n",
    "        pos_tag=nltk.pos_tag(word)\n",
    "        lemma=WordNetLemmatizer()\n",
    "        for i in range(len(pos_tag)):\n",
    "            if pos_tag[i][1]=='RB':\n",
    "                    lemma.lemmatize(word[i],pos=\"a\")\n",
    "            elif pos_tag[i][1]=='VBG':\n",
    "                lemma.lemmatize(word[i],pos=\"v\")\n",
    "            elif pos_tag[i][1]==\"NN\":\n",
    "                lemma.lemmatize(word[i],pos=\"n\")\n",
    "            else:          \n",
    "                lemma.lemmatize(word[i])\n",
    "\n",
    "\n",
    "        # calculating Positive score\n",
    "        positive_score=[]\n",
    "        for i in word:\n",
    "            if i in positive_words:\n",
    "                positive_score.append(1)\n",
    "        positive_score=sum(positive_score)\n",
    "\n",
    "        # Calculating Negative score\n",
    "        negative_score=[]\n",
    "        for i in word:\n",
    "            if i in negative_words:\n",
    "                negative_score.append(1)\n",
    "        negative_score=sum(negative_score)\n",
    "\n",
    "        #Calculating Polarity score + Word count\n",
    "        Polarity_Score = (positive_score-negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "        Polarity_Score \n",
    "        Word_Count=len(word)\n",
    "\n",
    "        # Syllable count per word + complex word count\n",
    "        vowels=[\"a\",\"e\",\"i\",\"o\",\"u\"]\n",
    "        no=[\"es\",\"ed\"]\n",
    "        Complex_word_count=[]\n",
    "        for i in word:\n",
    "            Syllable_Count_Per_Word=[]\n",
    "            for j in i:\n",
    "                if j in vowels and j not in no:\n",
    "                    Syllable_Count_Per_Word.append(1)\n",
    "            if sum(Syllable_Count_Per_Word)>2:\n",
    "                Complex_word_count.append(1)\n",
    "        Complex_word_count=sum(Complex_word_count)\n",
    "\n",
    "\n",
    "        # Calculationg Average Sentence length\n",
    "        sent=sent_tokenize(txt2)\n",
    "        avg_sentence_length=len(word)/len(sent)\n",
    "\n",
    "        #Percentage_of Complex words\n",
    "        Percentage_of_Complex_words = Complex_word_count/len(word) \n",
    "\n",
    "        #Fog_index\n",
    "        Fog_Index= 0.4*(avg_sentence_length + Percentage_of_Complex_words)\n",
    "\n",
    "        #9Average Word Length\n",
    "        Average_Word_Length=round(sum([len(i) for i in tokens])/len(tokens),2)\n",
    "        Average_Word_Length\n",
    "\n",
    "        # Calculating uncertaininty score \n",
    "        uncertainity_score=[]\n",
    "        for i in word:\n",
    "            if i in str(uncertainity[\"Word\"]).lower():\n",
    "                uncertainity_score.append(1)\n",
    "        uncertainity_score=sum(uncertainity_score)\n",
    "\n",
    "        # Calculating constraining score\n",
    "        constraining_score=[]\n",
    "        for i in word:\n",
    "            if i in str(constraining_dictionary[\"Word\"]).lower():\n",
    "                constraining_score.append(1)\n",
    "        constraining_score=sum(constraining_score)\n",
    "\n",
    "        #positive word proportion\n",
    "        positive_word_prportion=positive_score/len(word)\n",
    "\n",
    "        #negative word proportion\n",
    "        negative_word_proportion=negative_score/len(word)\n",
    "\n",
    "        #constraining word proportion\n",
    "        constraining_word_prportion=constraining_score/len(word)\n",
    "\n",
    "        #uncertainity word proportion\n",
    "        uncertainity_word_prportion=uncertainity_score/len(word)\n",
    "\n",
    "\n",
    "        #apending all the values inside the variables\n",
    "        positive_score1.append(positive_score), negative_score1.append(negative_score), polarity_score1.append(Polarity_Score),\n",
    "        average_sentence_length1.append(avg_sentence_length), percentage_of_complex_words1.append(Percentage_of_Complex_words),\n",
    "        fog_index1.append(Fog_Index), complex_word_count1.append(Complex_word_count), word_count1.append(Word_Count),\n",
    "        uncertainty_score1.append(uncertainity_score), constraining_score1.append(constraining_score),\n",
    "        positive_word_proportion1.append(positive_word_prportion), negative_word_proportion1.append(negative_word_proportion),\n",
    "        uncertainty_word_proportion1.append(uncertainity_word_prportion), constraining_word_proportion1.append(uncertainity_word_prportion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12504.0</td>\n",
       "      <td>44303.0</td>\n",
       "      <td>-0.559773</td>\n",
       "      <td>326.216092</td>\n",
       "      <td>0.090709</td>\n",
       "      <td>130.522720</td>\n",
       "      <td>12872.0</td>\n",
       "      <td>141904.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1798.0</td>\n",
       "      <td>0.088116</td>\n",
       "      <td>0.312204</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>-0.548023</td>\n",
       "      <td>29.666667</td>\n",
       "      <td>0.366292</td>\n",
       "      <td>12.013184</td>\n",
       "      <td>163.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.089888</td>\n",
       "      <td>0.307865</td>\n",
       "      <td>0.035955</td>\n",
       "      <td>0.035955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49039.0</td>\n",
       "      <td>164412.0</td>\n",
       "      <td>-0.540513</td>\n",
       "      <td>328.364300</td>\n",
       "      <td>0.089943</td>\n",
       "      <td>131.381697</td>\n",
       "      <td>44508.0</td>\n",
       "      <td>494845.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>7583.0</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.332249</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8005.0</td>\n",
       "      <td>28566.0</td>\n",
       "      <td>-0.562221</td>\n",
       "      <td>335.941176</td>\n",
       "      <td>0.086795</td>\n",
       "      <td>134.411189</td>\n",
       "      <td>7931.0</td>\n",
       "      <td>91376.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>0.087605</td>\n",
       "      <td>0.312620</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68018.0</td>\n",
       "      <td>153829.0</td>\n",
       "      <td>-0.386803</td>\n",
       "      <td>65.591516</td>\n",
       "      <td>0.076134</td>\n",
       "      <td>26.267060</td>\n",
       "      <td>19071.0</td>\n",
       "      <td>250494.0</td>\n",
       "      <td>82626.0</td>\n",
       "      <td>50541.0</td>\n",
       "      <td>0.271535</td>\n",
       "      <td>0.614103</td>\n",
       "      <td>0.329852</td>\n",
       "      <td>0.329852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33023.0</td>\n",
       "      <td>83658.0</td>\n",
       "      <td>-0.433961</td>\n",
       "      <td>126.529464</td>\n",
       "      <td>0.110914</td>\n",
       "      <td>50.656151</td>\n",
       "      <td>15718.0</td>\n",
       "      <td>141713.0</td>\n",
       "      <td>32543.0</td>\n",
       "      <td>23329.0</td>\n",
       "      <td>0.233027</td>\n",
       "      <td>0.590334</td>\n",
       "      <td>0.229640</td>\n",
       "      <td>0.229640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>174.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>-0.538462</td>\n",
       "      <td>2.453771</td>\n",
       "      <td>0.524046</td>\n",
       "      <td>1.191127</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.086267</td>\n",
       "      <td>0.287556</td>\n",
       "      <td>0.019336</td>\n",
       "      <td>0.019336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>172.0</td>\n",
       "      <td>513.0</td>\n",
       "      <td>-0.497810</td>\n",
       "      <td>2.419589</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>1.170734</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.085957</td>\n",
       "      <td>0.256372</td>\n",
       "      <td>0.024488</td>\n",
       "      <td>0.024488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>998.0</td>\n",
       "      <td>4339.0</td>\n",
       "      <td>-0.626007</td>\n",
       "      <td>22.169211</td>\n",
       "      <td>0.552999</td>\n",
       "      <td>9.088884</td>\n",
       "      <td>9636.0</td>\n",
       "      <td>17425.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>0.057274</td>\n",
       "      <td>0.249010</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>0.009756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>250.0</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>-0.694750</td>\n",
       "      <td>21.735043</td>\n",
       "      <td>0.571766</td>\n",
       "      <td>8.922723</td>\n",
       "      <td>2908.0</td>\n",
       "      <td>5086.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.049155</td>\n",
       "      <td>0.272906</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>0.009438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2           3         4           5        6  \\\n",
       "0  12504.0   44303.0 -0.559773  326.216092  0.090709  130.522720  12872.0   \n",
       "1     40.0     137.0 -0.548023   29.666667  0.366292   12.013184    163.0   \n",
       "2  49039.0  164412.0 -0.540513  328.364300  0.089943  131.381697  44508.0   \n",
       "3   8005.0   28566.0 -0.562221  335.941176  0.086795  134.411189   7931.0   \n",
       "4  68018.0  153829.0 -0.386803   65.591516  0.076134   26.267060  19071.0   \n",
       "5  33023.0   83658.0 -0.433961  126.529464  0.110914   50.656151  15718.0   \n",
       "6    174.0     580.0 -0.538462    2.453771  0.524046    1.191127   1057.0   \n",
       "7    172.0     513.0 -0.497810    2.419589  0.507246    1.170734   1015.0   \n",
       "8    998.0    4339.0 -0.626007   22.169211  0.552999    9.088884   9636.0   \n",
       "9    250.0    1388.0 -0.694750   21.735043  0.571766    8.922723   2908.0   \n",
       "\n",
       "          7        8        9        10        11        12        13  \n",
       "0  141904.0     57.0   1798.0  0.088116  0.312204  0.000402  0.000402  \n",
       "1     445.0     16.0     25.0  0.089888  0.307865  0.035955  0.035955  \n",
       "2  494845.0    275.0   7583.0  0.099100  0.332249  0.000556  0.000556  \n",
       "3   91376.0     53.0   1210.0  0.087605  0.312620  0.000580  0.000580  \n",
       "4  250494.0  82626.0  50541.0  0.271535  0.614103  0.329852  0.329852  \n",
       "5  141713.0  32543.0  23329.0  0.233027  0.590334  0.229640  0.229640  \n",
       "6    2017.0     39.0     46.0  0.086267  0.287556  0.019336  0.019336  \n",
       "7    2001.0     49.0     59.0  0.085957  0.256372  0.024488  0.024488  \n",
       "8   17425.0    170.0    298.0  0.057274  0.249010  0.009756  0.009756  \n",
       "9    5086.0     48.0     71.0  0.049155  0.272906  0.009438  0.009438  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables=[positive_score1,\n",
    "    negative_score1,\n",
    "    polarity_score1,\n",
    "    average_sentence_length1,\n",
    "    percentage_of_complex_words1,\n",
    "    fog_index1,\n",
    "    complex_word_count1,\n",
    "    word_count1,\n",
    "    uncertainty_score1,\n",
    "    constraining_score1,\n",
    "    positive_word_proportion1,\n",
    "    negative_word_proportion1,\n",
    "    uncertainty_word_proportion1,\n",
    "    constraining_word_proportion1]\n",
    "output=pd.DataFrame(variables)\n",
    "output=output.T\n",
    "c=output\n",
    "output.to_csv(r\"C:\\Mba notes\\novels\\Intern Dataset\\Text Analysis\\output_files\\output(0-20).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=\"C:/Mba notes/novels/Intern Dataset/Text Analysis/output_files\"\n",
    "join=os.listdir(path)\n",
    "final_path=[]\n",
    "for i in join:\n",
    "    fp=os.path.join(path,i)\n",
    "    final_path.append(fp)\n",
    "dfs=[]\n",
    "for i in final_path:\n",
    "    f=pd.read_csv(i)\n",
    "    dfs.append(f)\n",
    "data=pd.concat(dfs,ignore_index=True)\n",
    "output=pd.read_csv(r\"C:\\Mba notes\\novels\\Intern Dataset\\Text Analysis\\output.csv\")\n",
    "output_data_stucture=pd.read_excel(r\"C:\\Mba notes\\novels\\Intern Dataset\\Text Analysis\\Structure.xlsx\")\n",
    "output_data_stucture=output_data_stucture.drop([\"CIK\",\"CONAME\",\"FYRMO\",\"FDATE\",\"FORM\",\"SECFNAME\",\"constraining_words_whole_report\"],1)\n",
    "output.columns=output_data_stucture.columns\n",
    "final_output=pd.concat([url_data,output],1)\n",
    "final_output[\"constraining_words_whole_report\"]=final_output[\"word_count\"]/final_output[\"constraining_score\"]\n",
    "final_output.to_csv(r\"C:\\Mba notes\\novels\\Intern Dataset\\Text Analysis\\final_output_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
